@article{Sanchez-vives2005,
abstract = {Immersive virtual environments can break the deep everyday connection between where our senses tell us that we are and where we actually are located and whom we are with. ‘Presence research' studies the phenomenon of acting and feeling that we are in the world created by computer displays. We argue that presence is a phenomenon worthy of study by neuroscientists and may help towards the study of consciousness, since it may be regarded as consciousness within a restricted domain.},
author = {Sanchez-vives, Maria V. and Slater, Mel},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Presence{\_}Consciousness{\_}Jan12{\_}2004.pdf:pdf},
journal = {Nature Reviews Neuroscience},
number = {10},
pages = {332},
title = {{From Presence Towards Consciousness}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.99.5596{\&}rep=rep1{\&}type=pdf},
volume = {6},
year = {2005}
}
@article{Martinez2017,
abstract = {Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state-of-the-art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which result in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.},
archivePrefix = {arXiv},
arxivId = {1705.02445},
author = {Martinez, Julieta and Black, Michael J. and Romero, Javier},
doi = {10.1109/CVPR.2017.497},
eprint = {1705.02445},
file = {:Users/Christoph/Downloads/Martinez{\_}On{\_}Human{\_}Motion{\_}CVPR{\_}2017{\_}paper.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {4674--4683},
title = {{On human motion prediction using recurrent neural networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Schwind2018,
abstract = {Hand tracking and haptics are gaining more importance as key technologies of virtual reality (VR) systems. For designing such systems, it is fundamental to understand how the appearance of the virtual hands influences user experience and how the human brain integrates vision and haptics. However, it is currently unknown whether multi-sensory integration of visual and haptic feedback can be influenced by the appearance of virtual hands in VR. We performed a user study in VR to gain insight into the effect of hand appearance on how the brain combines visual and haptic signals using a cue-conflict paradigm. In this paper, we show that the detection of surface irregularities (bumps and holes) sensed by eyes and hands is affected by the rendering of avatar hands. However, sensitivity changes do not correlate with the degree of perceived limb ownership. Qualitative feedback provides insights into potentially distracting cues in visual-haptic integration.},
author = {Schwind, Valentin and Lin, Lorraine and {Di Luca}, Massimiliano and J{\"{o}}rg, Sophie and Hillis, James},
doi = {10.1145/3225153.3225158},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/3225153.3225158.pdf:pdf},
isbn = {9781450358941},
journal = {Proceedings - SAP 2018: ACM Symposium on Applied Perception},
keywords = {Avatars,Haptic perception,Haptics,Virtual body-ownership,Virtual hands,Virtual reality,Visual-haptic integration},
title = {{Touch with foreign hands: The effect of virtual hand appearance on visual-haptic integration}},
year = {2018}
}
@article{Lin2016,
abstract = {How does the appearance of a virtual hand affect own-body perception? Previous studies have compared either two or three hand models at a time, with their appearances limited to realistic hands and abstract or simple objects. To investigate the effects of different realisms, render styles, and sensitivities to pain on the virtual hand illusion (VHI), we conduct two studies in which participants take on controllable hand models with six distinct appearances. We collect questionnaire data and comments regarding responses to impacts and threats to assess differences in the strength of the VHI. Our findings indicate that an illusion can be created for any model for some participants, but that the effect is perceived weakest for a non-anthropomorphic block model and strongest for a realistic human hand model in direct comparison. We furthermore find that the responses to our experiments highly vary between participants.},
author = {Lin, Lorraine and J{\"{o}}rg, Sophie},
doi = {10.1145/2931002.2931006},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/2931002.2931006.pdf:pdf},
isbn = {9781450343831},
journal = {Proceedings of the ACM Symposium on Applied Perception, SAP 2016},
keywords = {Body awareness,Body ownership,Presence,Realism,Rubber hand illusion,Virtual character,Virtual environments,Virtual hand illusion,Virtual reality},
pages = {69--76},
title = {{Need a hand? How appearance affects the virtual hand illusion}},
year = {2016}
}
@article{Schubert2002,
author = {Schubert, Thomas and Regenbrecht, Holger},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Wer{\_}hat{\_}Angst{\_}vor{\_}virtueller{\_}Realitat{\_}Angst{\_}Therap.pdf:pdf},
journal = {Virtuelle Realit{\"{a}}ten},
pages = {225--274},
title = {{Wer hat Angst vor virtueller Realit{\"{a}}t ? Angst , Therapie und Pr{\"{a}}senz in virtuellen Welten Angst , Therapie und Pr{\"{a}}senz in VR}},
year = {2002}
}
@article{Schubert1999,
abstract = {Schubert, T.W., Friedman, F., {\&} Regenbrecht, H.T. (1999). Embodied presence in virtual environments. In: Paton R, Neilson I (Eds.). Visual representations and interpretations. Springer-Verlag, London, pp. 268–278.},
author = {Schubert, Thomas and Friedmann, Frank and Regenbrecht, Holger},
doi = {10.1007/978-1-4471-0563-3_30},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/embodied presence in vr.pdf:pdf},
journal = {Visual Representations and Interpretations},
pages = {269--278},
title = {{Embodied Presence in Virtual Environments}},
year = {1999}
}
@article{MacKenzie1989,
abstract = {Fitts' law is an information-theoretic view of human motor behavior developed from Shannon's Theorem 17, a fundamental theorem of communications systems. Using data from Fitts' original experiments, we demonstrate that Fitts' choice of an equation that deviates slightly from the underlying principle is perhaps unfounded, and that the relationship is improved by using an exact adaptation of Shannon's equation. {\textcopyright} 1989 by The Helen Dwight Reid Educational Foundation.},
author = {{MacKenzie}, I. Scott},
doi = {10.1080/00222895.1989.10735486},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/note information theoretic basis for fitts.pdf:pdf},
issn = {19401027},
journal = {Journal of Motor Behavior},
number = {3},
pages = {323--330},
title = {{A note on the information-theoretic basis for fitts' law}},
volume = {21},
year = {1989}
}
@article{ISO2012,
author = {ISO},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/preview{\_}ISO+TS+9241-411-2012.pdf:pdf},
title = {{Evaluation methods for the design of physical input devices}},
year = {2012}
}
@article{Gianaros2001,
abstract = {A limited number of attempts have been made to develop a questionnaire that assesses the experience of motion sickness. Further, many available questionnaires quantify motion sickness as a unidimensional construct. Exploratory and confirmatory factor analyses of motion sickness descriptors were used to derive and verify four dimensions of motion sickness, which were defined as gastrointestinal, central, peripheral, and sopite-related. These dimensions of motion sickness were then used to construct a motion sickness assessment questionnaire (MSAQ) that was administered to individuals who were exposed to a rotating optokinetic drum. Total scores from the MSAQ correlated strongly with overall scores from the Pensacola Diagnostic Index (r = 0.81, p {\textless} 0.001) and the Nausea Profile (r = 0.92, p {\textless} 0.001). The MSAQ is a valid instrument for the assessment of motion sickness. In addition, the MSAQ may be used to assess motion sickness as a multidimensional rather than unidimensional construct.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Gianaros, Peter J. and R., Muth Eric and Mordkoff, Jonathan Toby and Levine, Max E and Stem, Robert M.},
doi = {10.1038/jid.2014.371},
eprint = {NIHMS150003},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/A{\_}Questionnaire{\_}for{\_}the{\_}Assessment{\_}of{\_}the{\_}Multiple.pdf:pdf},
isbn = {6176321972},
issn = {15378276},
journal = {Aviation Space and Environmental Medicine},
keywords = {epiblast,gfp fusion,histone h2b-,icm,lineage specification,live imaging,mouse blastocyst,pdgfr $\alpha$,primitive endoderm},
number = {2},
pages = {115--119},
pmid = {1000000221},
title = {{A Questionnaire for the Assessment of the Multiple Dimensions of Motion Sickness}},
volume = {72},
year = {2001}
}
@article{Faul2007,
abstract = {G*Power (Erdfelder, Faul, {\&} Buchner, 1996) was designed as a general stand-alone power analysis program for statistical tests commonly used in social and behavioral research. G*Power 3 is a major extension of, and improvement over, the previous versions. It runs on widely used computer platforms (i.e., Windows XP, Windows Vista, and Mac OS X 10.4) and covers many different statistical tests of the t, F, and $\chi$2 test families. In addition, it includes power analyses for z tests and some exact tests. G*Power 3 provides improved effect size calculators and graphic options, supports both distribution-based and design-based input modes, and offers all types of power analyses in which users might be interested. Like its predecessors, G*Power 3 is free. Copyright 2007 Psychonomic Society, Inc.},
author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert Georg and Buchner, Axel},
doi = {10.3758/BF03193146},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/GPower3-BRM-Paper.pdf:pdf},
issn = {1554351X},
journal = {Behavior Research Methods},
number = {2},
pages = {175--191},
pmid = {17695343},
title = {{G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences}},
volume = {39},
year = {2007}
}
@article{Erdfelder2009,
abstract = {G*Power is a free power analysis program for a variety of statistical tests. We present extensions and improvements of the version introduced by Faul, Erdfelder, Lang, and Buchner (2007) in the domain of correlation and regression analyses. In the new version, we have added procedures to analyze the power of tests based on (1) single-sample tetrachoric correlations, (2) comparisons of dependent correlations, (3) bivariate linear regression, (4) multiple linear regression based on the random predictor model, (5) logistic regression, and (6) Poisson regression. We describe these new features and provide a brief introduction to their scope and handling. {\textcopyright} 2009 The Psychonomic Society. Inc.},
author = {Erdfelder, Edgar and FAul, Franz and Buchner, Axel and Lang, Albert Georg},
doi = {10.3758/BRM.41.4.1149},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/GPower31-BRM-Paper.pdf:pdf},
issn = {1554351X},
journal = {Behavior Research Methods},
number = {4},
pages = {1149--1160},
pmid = {19897823},
title = {{Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses}},
volume = {41},
year = {2009}
}
@article{Braun2018,
abstract = {Usually, we do not question that we possess a body and act upon the world. This pre-reflective awareness of being a bodily and agentive self can, however, be disrupted by different clinical conditions. Whereas sense of ownership (SoO) describes the feeling of mineness toward one's own body parts, feelings or thoughts, sense of agency (SoA) refers to the experience of initiating and controlling an action. Although SoA and SoO naturally coincide, both experiences can also be made in isolation. By using many different experimental paradigms, both experiences have been extensively studied over the last years. This review introduces both concepts, with a special focus also onto their interplay. First, current experimental paradigms, results and neurocognitive theories about both concepts will be presented and then their clinical and therapeutic relevance is discussed.},
author = {Braun, Niclas and Debener, Stefan and Spychala, Nadine and Bongartz, Edith and S{\"{o}}r{\"{o}}s, Peter and M{\"{u}}ller, Helge H.O. and Philipsen, Alexandra},
doi = {10.3389/fpsyg.2018.00535},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/agency and ownership.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Alien hand syndrome,Asomatognosia,Limb-ownership,Phenomenal transparency,Rubber hand illusion,Sense of agency,Sense of ownership,Virtual reality therapy},
number = {APR},
pages = {1--17},
title = {{The senses of agency and ownership: A review}},
volume = {9},
year = {2018}
}
@article{Botvinick2006,
author = {Botvinick, Matthew and Cohen, Jonathan},
doi = {10.1016/j.jalz.2006.05.522},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/rubber hand.pdf:pdf},
issn = {1552-5260},
journal = {Nature},
number = {1},
pages = {756},
title = {{Rubber hands ‘feel' touch that eyes see}},
volume = {391},
year = {2006}
}
@article{Perez-Marcos2009,
abstract = {The apparently stable brain representation of our bodies is easily challenged. We have recently shown that the illusion of ownership of a three-dimensional virtual hand can be evoked through synchronous tactile stimulation of a person's hidden real hand and that of the virtual hand. This reproduces the well-known rubber-hand illusion, but in virtual reality. Here we show that some aspects of the illusion can also occur through motor imagery used to control movements of a virtual hand. When movements of the virtual hand followed motor imagery, the illusion of ownership of the virtual hand was evoked and muscle activity measured through electromyogram correlated with movements of the virtual arm. Using virtual bodies has a great potential in the fields of physical and neural rehabilitation, making the understanding of ownership of a virtual body highly relevant. {\textcopyright} 2009 Wolters Kluwer Health | Lippincott Williams {\&} Wilkins.},
author = {Perez-Marcos, Daniel and Slater, Mel and Sanchez-Vives, Maria V.},
doi = {10.1097/WNR.0b013e32832a0a2a},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Inducing{\_}a{\_}virtual{\_}hand{\_}ownership{\_}illusi.pdf:pdf},
issn = {09594965},
journal = {NeuroReport},
keywords = {Body illusion,Body perception,Body representation,Body scheme,Brain-computer interface,Rehabilitation,Virtual environments,Virtual reality},
number = {6},
pages = {589--594},
pmid = {19938302},
title = {{Inducing a virtual hand ownership illusion through a brain-computer interface}},
volume = {20},
year = {2009}
}
@article{Kilteni2012,
abstract = {What does it feel like to own, to control, and to be inside a body? The multidimensional nature of this experience together with the continuous presence of one's biological body, render both theoretical and experimental approaches problematic. Nevertheless, exploitation of immersive virtual reality has allowed a reframing of this question to whether it is possible to experience the same sensations towards a virtual body inside an immersive virtual environment as toward the biological body, and if so, to what extent. The current paper addresses these issues by referring to the Sense of Embodiment (SoE). Due to the conceptual confusion around this sense, we provide a working definition which states that SoE consists of three subcomponents: the sense of self-location, the sense of agency, and the sense of body ownership. Under this proposed structure, measures and experimental manipulations reported in the literature are reviewed and related challenges are outlined. Finally, future experimental studies are proposed to overcome those challenges, toward deepening the concept of SoE and enhancing it in virtual applications. {\textcopyright} 2012 by the Massachusetts Institute of Technology.},
author = {Kilteni, Konstantina and Groten, Raphaela and Slater, Mel},
doi = {10.1162/PRES_a_00124},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/sense of embodiment in vr.pdf:pdf},
issn = {15313263},
journal = {Presence: Teleoperators and Virtual Environments},
number = {4},
pages = {373--387},
title = {{The Sense of Embodiment in virtual reality}},
volume = {21},
year = {2012}
}
@article{Tsakiris2006,
abstract = {We investigated how motor agency in the voluntary control of body movement influences body awareness. In the Rubber Hand Illusion (RHI), synchronous tactile stimulation of a rubber hand and the participant's hand leads to a feeling of the rubber hand being incorporated in the participant's own body. One quantifiable behavioural correlate of the illusion is an induced shift in the perceived location of the participant's hand towards the rubber hand. Previous studies showed that the induced changes in body awareness are local and fragmented: the proprioceptive drift is largely restricted to the stimulated finger. In the present study, we investigated whether active and passive movements, rather than tactile stimulation, would lead to similarly fragmented body awareness. Participants watched a projected image of their hand under three conditions: active finger movement, passive finger movement, and tactile stimulation. Visual feedback was either synchronous or asynchronous with respect to stimulation of the hand. A significant overall RHI, defined as greater drifts following synchronous than asynchronous stimulation, was found in all cases. However, the distribution of the RHI across stimulated and non-stimulated fingers depended on the kind of stimulation. Localised proprioceptive drifts, specific to the stimulated finger, were found for tactile and passive stimulation. Conversely, during active movement of a single digit, the proprioceptive drifts were not localised to that digit, but were spread across the whole hand. Whereas a purely proprioceptive sense of body-ownership is local and fragmented, the motor sense of agency integrates distinct body-parts into a coherent, unified awareness of the body. {\textcopyright} 2005 Elsevier Inc. All rights reserved.},
author = {Tsakiris, Manos and Prabhu, Gita and Haggard, Patrick},
doi = {10.1016/j.concog.2005.09.004},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Having{\_}a{\_}Body{\_}Versus{\_}Moving{\_}Your{\_}Body{\_}Ho.pdf:pdf},
issn = {10538100},
journal = {Consciousness and Cognition},
keywords = {Action,Active movement,Agency,Body-awareness,Body-ownership,Passive movement,Proprioception,Rubber hand illusion,Sense of agency,Touch},
number = {2},
pages = {423--432},
title = {{Having a body versus moving your body: How agency structures body-ownership}},
volume = {15},
year = {2006}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/adam.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1016/0010-4361(73)90803-3},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/JMLRdropout.pdf:pdf},
issn = {0031918X},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
number = {1},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Schwind2019a,
abstract = {Virtual reality (VR) is becoming more and more ubiquitous to interact with digital content and often requires renderings of avatars as they enable improved spatial localization and high levels of presence. Previous work shows that visual-haptic integration of virtual avatars depends on body ownership and spatial localization in VR. However, there are different conclusions about how and which stimuli of the own appearance are integrated into the own body scheme. In this work, we investigate if systematic changes of model and texture of a users' avatar affect the input performance measured in a two-dimensional Fitts' law target selection task. Interestingly, we found that the throughput remained constant between our conditions and that neither model nor texture of the avatar significantly affected the average duration to complete the task even when participants felt different levels of presence and body ownership. In line with previous work, we found that the illusion of virtual limb-ownership does not necessarily correlate to the degree to which vision and haptics are integrated into the own body scheme. Our work supports findings indicating that body ownership and spatial localization are potentially independent mechanisms in visual-haptic integration.},
author = {Schwind, Valentin and Leusmann, Jan and Henze, Niels},
doi = {10.1145/3340764.3340769},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/2019-muc-fittsinvr.pdf:pdf},
isbn = {9781450371988},
journal = {ACM International Conference Proceeding Series},
keywords = {Avatars,Depth cues,Fitts' law,Virtual reality,Visual-haptic integration},
pages = {211--222},
title = {{Understanding visual-haptic integration of avatar hands using a Fitts' law task in virtual reality}},
year = {2019}
}
@article{Kohli2010,
abstract = {There is an increasing interest in deployable virtual military training systems. Haptic feedback for these training systems can enable users to interact more naturally with the training environment, but is difficult to deploy. Passive haptic feedback is very compelling, but it is also inflexible. Changes made to virtual objects can require time-consuming changes to their physical passive-haptic counterparts. This poster explores the possibility of mapping many differently shaped virtual objects onto one physical object by warping virtual space and exploiting the dominance of the visual system. A first implementation that maps different virtual objects onto dynamically captured physical geometry is presented, and potential applications to deployable military trainers are discussed. {\textcopyright}2010 IEEE.},
author = {Kohli, Luv},
doi = {10.1109/3DUI.2010.5444703},
file = {:Users/Christoph/Downloads/kohli2010.pdf:pdf},
isbn = {9781424468447},
journal = {3DUI 2010 - IEEE Symposium on 3D User Interfaces 2010, Proceedings},
pages = {129--130},
title = {{Redirected touching: Warping space to remap passive haptics}},
year = {2010}
}
@article{Agarap2018,
abstract = {We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer {\$}h{\_}{\{}n - 1{\}}{\$} in a neural network, then multiply it by weight parameters {\$}\backslashtheta{\$} to get the raw scores {\$}o{\_}{\{}i{\}}{\$}. Afterwards, we threshold the raw scores {\$}o{\_}{\{}i{\}}{\$} by {\$}0{\$}, i.e. {\$}f(o) = \backslashmax(0, o{\_}{\{}i{\}}){\$}, where {\$}f(o){\$} is the ReLU function. We provide class predictions {\$}\backslashhat{\{}y{\}}{\$} through argmax function, i.e. argmax {\$}f(x){\$}.},
archivePrefix = {arXiv},
arxivId = {1803.08375},
author = {Agarap, Abien Fred},
eprint = {1803.08375},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Relu.pdf:pdf},
keywords = {artificial intelligence,artificial neural networks,classification,con-,deep learning,deep neural networks,feed-forward neural network,machine learning,rectified linear,volutional neural network},
number = {1},
pages = {2--8},
title = {{Deep Learning using Rectified Linear Units (ReLU)}},
url = {http://arxiv.org/abs/1803.08375},
year = {2018}
}
@article{Law2018,
author = {MacKenzie, I. Scott},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/hhci2018.pdf:pdf},
journal = {Handbook of human-computer interaction},
pages = {349--370},
title = {{Fitts' Law}},
year = {2018}
}
@inproceedings{Saad,
abstract = {In virtual reality (VR), head movement is tracked through inertial and optical sensors. Computation and communication times result in delays between measurements and updating of the new frame in the head mounted display(HMD). These delays result in problems, including motion sickness. We use recurrent and time delay neural networks to predict the head location and use it to calculate the new frame. A predictability analysis is used in designing the prediction system.},
author = {Saad, Emad W. and Caudell, Thomas P. and Wunsch, Donald C.},
booktitle = {IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No. 99CH36339)},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Predictive head tracking for virtual reality.pdf:pdf},
title = {{Predictive head tracking for virtual reality}},
volume = {6},
year = {1999}
}
@article{Aladagli2018,
author = {Aladagli, A. Deniz and Ekmekcioglu, Erhan and Kondoz, Ahmet and Jarnikov, Dmitri},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Predicting head trajectories in 360° virtual reality videos.pdf:pdf},
journal = {2017 International Conference on 3D Immersion, IC3D 2017 - Proceedings},
pages = {1--6},
title = {{Predicting Head Trajectories in 360 Virtual Reality Videos}},
volume = {7},
year = {2018}
}
@article{Himberg2009,
abstract = {Display lag in simulation environments with helmet-mounted displays causes a loss of immersion that degrades the value of virtual/augmented reality training simulators. Simulators use predictive tracking to compensate for display lag, preparing display updates based on the anticipated head motion. This paper proposes a new method for predicting head orientation using a delta quaternion (DQ)-based extended Kalman filter (EKF) and compares the performance to a quaternion EKF. The proposed framework operates on the change in quaternion between consecutive data frames (the DQ), which avoids the heavy computational burden of the quaternion motion equation. Head velocity is estimated from the DQ by an EKF and then used to predict future head orientation. We have tested the new framework with captured head motion data and compared it with the computationally expensive quaternion filter. Experimental results indicate that the proposed DQ method provides the accuracy of the quaternion method without the heavy computational burden. {\textcopyright} 2009 IEEE.},
author = {Himberg, H. and Motai, Y.},
doi = {10.1109/TSMCB.2009.2016571},
file = {:Users/Christoph/Documents/Uni/Semester 11$\backslash$:3/Forschungsseminar/GIT/FS{\_}2019{\_}FTIRT/DOCS/Paper{\_}/Head{\_}Orientation{\_}Prediction{\_}Delta{\_}Quaternions{\_}Vers.pdf:pdf},
issn = {10834419},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
keywords = {Compensation,Kalman filtering,Kalman filters,Prediction methods,Quaternions,Tracking,Virtual reality},
number = {6},
pages = {1382--1392},
title = {{Head Orientation Prediction: Delta Quaternions Versus Quaternions}},
volume = {39},
year = {2009}
}
@article{,
archivePrefix = {arXiv},
arxivId = {1712.00547},
eprint = {1712.00547},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2017 - IJCAI-17 Workshop on Explainable AI (XAI).pdf:pdf},
issn = {18737072},
title = {{IJCAI-17 Workshop on Explainable AI (XAI)}},
year = {2017}
}
@article{Katz2017,
author = {Katz, G. E. and Dullnig, D. and Davis, G.P. and Gentili, R. J. and Reggia, J.},
journal = {International Conference on Computational Science and Computational Intelligence (CSCI)},
pages = {772--778},
title = {{Autonomous causally-driven explanation of actions}},
year = {2017}
}
@article{Montavon2018,
abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
archivePrefix = {arXiv},
arxivId = {1706.07979},
author = {Montavon, Gr{\'{e}}goire and Samek, Wojciech and M{\"{u}}ller, Klaus Robert},
doi = {10.1016/j.dsp.2017.10.011},
eprint = {1706.07979},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Montavon, Samek, M{\"{u}}ller - 2018 - Methods for interpreting and understanding deep neural networks.pdf:pdf},
issn = {10512004},
journal = {Digital Signal Processing: A Review Journal},
keywords = {Activation maximization,Deep neural networks,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition},
pages = {1--15},
publisher = {Elsevier Inc.},
title = {{Methods for interpreting and understanding deep neural networks}},
url = {https://doi.org/10.1016/j.dsp.2017.10.011},
volume = {73},
year = {2018}
}
@article{Koo2015,
abstract = {This study explores, in the context of semi-autonomous driving, how the content of the verbalized message accompanying the car's autonomous action affects the driver's attitude and safety performance. Using a driving simulator with an auto-braking function, we tested different messages that provided advance explanation of the car's imminent autonomous action. Messages providing only “how” information describing actions (e.g., “The car is braking”) led to poor driving performance, whereas “why” information describing reasoning for actions (e.g., “Obstacle ahead”) was preferred by drivers and led to better driving performance. Providing both “how and why” resulted in the safest driving performance but increased negative feelings in drivers. These results suggest that, to increase overall safety, car makers need to attend not only to the design of autonomous actions but also to the right way to explain these actions to the drivers.},
author = {Koo, Jeamin and Kwac, Jungsuk and Ju, Wendy and Steinert, Martin and Leifer, Larry and Nass, Clifford},
doi = {10.1007/s12008-014-0227-2},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Koo et al. - 2015 - Why did my car just do that Explaining semi-autonomous driving actions to improve driver understanding, trust, and p.pdf:pdf},
issn = {19552505},
journal = {International Journal on Interactive Design and Manufacturing},
keywords = {Car–driver interaction,Feedforward alerts,Semi-autonomous driving},
title = {{Why did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance}},
year = {2015}
}
@article{Duffy2013,
abstract = {Driverless cars have made the jump from fantasy to the physical realm. Technology has evolved to the point where autonomous cars will be a common sight in the very near future. The benefits of autonomous cars are plentiful: increased safety for car passengers, who no longer have to fear drunk, reckless, or distracted drivers, increased productivity for passengers who can use the travel time to accomplish tasks, decreased reliance on fuel as the cars often incorporate solar panels and automatically adjust speed to maximize fuel efficiency, and decreased traffic congestion as the cars can identify upcoming trouble spots and take alternate routes to avoid delay. However, this innovative technology brings with it an unaddressed legal issue: how will legal liability be assessed when these cars collide with other cars, pedestrians, or property? Current law surrounding liability for automobile accidents largely bases liability on the actions of the driver. Similarly, looking to the liability law governing computers does not address the issue either, as the laws base liability on the actions of the operator of the computer system, and the scant laws related to autonomous computer systems apply only to commercial transactions. This article proposes that the solution to this legal issue lies in treating autonomous cars like man's best friend, the dog. Dogs and computers are both treated as chattel under tort law, and are similar in that they can act independently, yet are considered property of another. The laws governing canine ownership show that applying strict liability to autonomous car owners accomplishes the dual purpose of fairly assessing liability without hampering the widespread adoption of this marvelous technology.},
author = {Duffy, Sophia and Hopkins, Jamie Patrick},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Duffy, Hopkins - 2013 - Sit, Stay, Drive The Future of Autonomous Car Liability.pdf:pdf},
journal = {Science and Technology Law Review},
keywords = {Audi,Nevada,accident,autonomous,canine,car,dog,driverless,future,google,google car,law,liability,maps,street view,vehicle},
number = {3},
pages = {453},
title = {{Sit, Stay, Drive: The Future of Autonomous Car Liability}},
volume = {16},
year = {2013}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {1602.04938},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Why should i trust you Explaining the predictions of any classifier.pdf:pdf},
isbn = {9781450342322},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135--1144},
title = {{"Why should i trust you?" Explaining the predictions of any classifier}},
volume = {13-17-Augu},
year = {2016}
}
@article{Madumal2019,
abstract = {Explainable Artificial Intelligence (XAI) systems need to include an explanation model to communicate the internal decisions, behaviours and actions to the interacting humans. Successful explanation involves both cognitive and social processes. In this paper we focus on the challenge of meaningful interaction between an explainer and an explainee and investigate the structural aspects of an interactive explanation to propose an interaction protocol. We follow a bottom-up approach to derive the model by analysing transcripts of different explanation dialogue types with 398 explanation dialogues. We use grounded theory to code and identify key components of an explanation dialogue. We formalize the model using the agent dialogue framework (ADF) as a new dialogue type and then evaluate it in a human-agent interaction study with 101 dialogues from 14 participants. Our results show that the proposed model can closely follow the explanation dialogues of human-agent conversations.},
archivePrefix = {arXiv},
arxivId = {1903.02409},
author = {Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
eprint = {1903.02409},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Madumal et al. - 2019 - A Grounded Interaction Protocol for Explainable Artificial Intelligence.pdf:pdf},
keywords = {Dialogue Model,Explainable AI,Human-Agent Interaction,Interpretable Machine Learning,acm reference format,dialogue model,explainable ai,human-agent interaction,interpretable machine learning},
number = {Aamas},
pages = {1033--1041},
title = {{A Grounded Interaction Protocol for Explainable Artificial Intelligence}},
url = {http://arxiv.org/abs/1903.02409},
year = {2019}
}
@article{Holzinger2019,
abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system. This article is categorized under: Fundamental Concepts of Data and Knowledge {\textgreater} Human Centricity and User Interaction.},
author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"{u}}ller, Heimo},
doi = {10.1002/widm.1312},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Holzinger et al. - 2019 - Causability and explainability of artificial intelligence in medicine.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine},
number = {4},
pages = {1--13},
title = {{Causability and explainability of artificial intelligence in medicine}},
volume = {9},
year = {2019}
}
@article{Petkovic2018,
abstract = {The goals of this workshop are to discuss challenges in explainability of current Machine Leaning and Deep Analytics (MLDA) used in biocomputing and to start the discussion on ways to improve it. We define explainability in MLDA as easy to use information explaining why and how the MLDA approach made its decisions. We believe that much greater effort is needed to address the issue of MLDA explainability because of: 1) the ever increasing use and dependence on MLDA in biocomputing including the need for increased adoption by non-MLD experts; 2) the diversity, complexity and scale of biocomputing data and MLDA algorithms; 3) the emerging importance of MLDA-based decisions in patient care, in daily research, as well as in the development of new costly medical procedures and drugs. This workshop aims to: a) analyze and challenge the current level of explainability of MLDA methods and practices in biocomputing; b) explore benefits of improvements in this area; and c) provide useful and practical guidance to the biocomputing community on how to address these challenges and how to develop improvements. The workshop format is designed to encourage a lively discussion with panelists to first motivate and understand the problem and then to define next steps and solutions needed to improve MLDA explainability.},
author = {Petkovic, Dragutin and Kobzik, Lester and Re, Christopher},
doi = {10.1142/9789813235533_0058},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Petkovic, Kobzik, Re - 2018 - Machine learning and deep analytics for biocomputing Call for better explainability.pdf:pdf},
issn = {23356936},
journal = {Pacific Symposium on Biocomputing},
keywords = {Explainability,Interpretability,Machine learning,Workshop},
number = {212669},
pages = {623--627},
pmid = {29218921},
title = {{Machine learning and deep analytics for biocomputing: Call for better explainability}},
volume = {0},
year = {2018}
}
@article{Ha2018,
abstract = { Explainability and accuracy of the machine learning algorithms usually laid on a trade-off relationship. Several algorithms such as deep-learning artificial neural networks have high accuracy but low explainability. Since there were only limited ways to access the learning and prediction processes in algorithms, researchers and users were not able to understand how the results were given to them. However, a recent project, explainable artificial intelligence (XAI) by DARPA, showed that AI systems can be highly explainable but also accurate. Several technical reports of XAI suggested ways of extracting explainable features and their positive effects on users; the results showed that explainability of AI was helpful to make users understand and trust the system. However, only a few studies have addressed why the explainability can bring positive effects to users. We suggest theoretical reasons from the attribution theory and anthropomorphism studies. Through a review, we develop three hypotheses: (1) causal attribution is a human nature and thus a system which provides casual explanation on their process will affect users to attribute the result of system; (2) Based on the attribution results, users will perceive the system as human-like and which will be a motivation of anthropomorphism; (3) The system will be perceived by the users through the anthropomorphism. We provide a research framework for designing causal explainability of an AI system and discuss the expected results of the research.},
author = {Ha, Taehyun and Lee, Sangwon and Kim, Sangyeon},
doi = {10.1145/3183654.3183683},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Ha, Lee, Kim - 2018 - Designing Explainability of an Artificial Intelligence System.pdf:pdf},
pages = {1--1},
title = {{Designing Explainability of an Artificial Intelligence System}},
volume = {18},
year = {2018}
}
@article{Wang2019,
author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y and States, United},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable AI.pdf:pdf},
isbn = {9781450359702},
keywords = {acm reference format,clinical decision making,decision making,explainable arti fi cial,explanations,intelligence,intelligibility},
pages = {1--15},
title = {{Designing Theory-Driven User-Centric Explainable AI}},
year = {2019}
}
@article{Anjomshoae2019,
abstract = {Humans are increasingly relying on complex systems that heavily adopts Artificial Intelligence (AI) techniques. Such systems are employed in a growing number of domains, and making them explainable is an impelling priority. Recently, the domain of eX-plainable Artificial Intelligence (XAI) emerged with the aims of fostering transparency and trustworthiness. Several reviews have been conducted. Nevertheless, most of them deal with data-driven XAI to overcome the opaqueness of black-box algorithms. Contributions addressing goal-driven XAI (e.g., explainable agency for robots and agents) are still missing. This paper aims at filling this gap, proposing a Systematic Literature Review. The main findings are (i) a considerable portion of the papers propose conceptual studies, or lack evaluations or tackle relatively simple scenarios; (ii) almost all of the studied papers deal with robots/agents explaining their behaviors to the human users, and very few works addressed inter-robot (inter-agent) explainability. Finally, (iii) while providing explanations to non-expert users has been outlined as a necessity, only a few works addressed the issues of personalization and context-awareness.},
author = {Anjomshoae, Sule and Najjar, Amro and Calvaresi, Davide and Fr{\"{a}}mling, Kary},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Anjomshoae et al. - 2019 - Explainable Agents and Robots Results from a Systematic Literature Review.pdf:pdf},
journal = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS '19)},
keywords = {Explainable AI,autonomous agents,explainable ai,goal-based XAI,goal-based xai,human-robot,human-robot interaction},
number = {Aamas},
pages = {1078--1088},
title = {{Explainable Agents and Robots: Results from a Systematic Literature Review}},
url = {www.ifaamas.org},
year = {2019}
}
@article{Beaton2018,
abstract = {Inside AI research and engineering communities, explainable artificial intelligence (XAI) is one of the most provocative and promising lines of AI research and development today. XAI has the potential to make expressible the context and domain-specific benefits of particular AI applications to a diverse and inclusive array of stakeholders and audiences. In addition, XAI has the potential to make AI benefit claims more deeply evidenced. Outside AI research and engineering communities, one of the most provocative and promising lines of research happening today is the work on "humanoid capital" at the edges of the social, behavioral, and economic sciences. Humanoid capital theorists renovate older discussions of "human capital" as part of trying to make calculable and provable the domain-specific capital value, value-adding potential, or relative worth (i.e., advantages and benefits) of different humanoid models over time. Bringing these two exciting streams of research into direct conversation for the first time is the larger goal of this landmark paper. The primary research contribution of the paper is to detail some of the key requirements for making humanoid robots explainable in capital terms using XAI approaches. In this regard, the paper not only brings two streams of provocative research into much-needed conversation but also advances both streams.},
author = {Beaton, Brian},
doi = {10.1145/3173386.3173391},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Beaton - 2018 - Crucial Answers about Humanoid Capital.pdf:pdf},
isbn = {9781450356152},
issn = {21672148},
journal = {ACM/IEEE International Conference on Human-Robot Interaction},
keywords = {capital,explainable artificial intelligence,humanoid robots,xai},
pages = {5--12},
title = {{Crucial Answers about Humanoid Capital}},
year = {2018}
}
@article{Schwind2019,
abstract = {Virtual Reality (VR) is gaining increasing importance in science, education, and entertainment. A fundamental characteristic of VR is creating presence, the experience of'being' or'acting', when physically situated in another place. Measuring presence is vital for VR research and development. It is typically repeatedly assessed through questionnaires completed after leaving a VR scene. Requiring participants to leave and re-enter the VR costs time and can cause disorientation. In this paper, we investigate the effect of completing presence questionnaires directly in VR. Thirty-six participants experienced two immersion levels and filled three standardized presence questionnaires in the real world or VR. We found no effect on the questionnaires' mean scores; however, we found that the variance of those measures significantly depends on the realism of the virtual scene and if the subjects had left the VR. The results indicate that, besides reducing a study's duration and reducing disorientation, completing questionnaires in VR does not change the measured presence but can increase the consistency of the variance.},
author = {Schwind, Valentin and Knierim, Pascal and Haas, Nico and Henze, Niels},
doi = {10.1145/3290605.3300590},
file = {:Users/Christoph/Downloads/2019-CHI-QuestionnairesInVR.pdf:pdf},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Evaluation,Presence,Questionnaire,Virtual reality},
pages = {1--12},
title = {{Using presence questionnaires in virtual reality}},
year = {2019}
}
@article{Gonzalez-Franco2018,
abstract = {Inside virtual reality, users can embody avatars that are collocated from a first-person perspective. When doing so, participants have the feeling that the own body has been substituted by the self-avatar, and that the new body is the source of the sensations. Embodiment is complex as it includes not only body ownership over the avatar, but also agency, co-location, and external appearance. Despite the multiple variables that influence it, the illusion is quite robust, and it can be produced even if the self-avatar is of a different age, size, gender, or race from the participant's own body. Embodiment illusions are therefore the basis for many social VR experiences and a current active research area among the community. Researchers are interested both in the body manipulations that can be accepted, as well as studying how different self-avatars produce different attitudinal, social, perceptual, and behavioral effects. However, findings suggest that despite embodiment being strongly associated with the performance and reactions inside virtual reality, the extent to which the illusion is experienced varies between participants. In this paper, we review the questionnaires used in past experiments and propose a standardized embodiment questionnaire based on 25 questions that are prevalent in the literature. We encourage future virtual reality experiments that include first-person virtual avatars to administer this questionnaire in order to evaluate the degree of embodiment.},
author = {Gonzalez-Franco, Mar and Peck, Tabitha C.},
doi = {10.3389/frobt.2018.00074},
file = {:Users/Christoph/Downloads/frobt-05-00074.pdf:pdf},
issn = {22969144},
journal = {Frontiers Robotics AI},
keywords = {Avatars,Body ownership illusion,Embodiment,Questionnaires,Virtual reality},
number = {JUN},
pages = {1--9},
title = {{Avatar embodiment. Towards a standardized questionnaire}},
volume = {5},
year = {2018}
}
@article{Schubert2001,
abstract = {We wish to thank the members of the igroup for their help in conducting this research, VRT GmbH for their financial support, and Sven Waldzus and three anonymous reviewers for helpful comments on earlier drafts of this article.},
author = {Schubert, Thomas and Regenbrecht, Holger and Friedmann, Frank},
file = {:Users/Christoph/Desktop/schubert2001.pdf:pdf},
journal = {Presence: Teleoperators and Virtual Environments},
number = {3},
pages = {266----281},
title = {{The experience of presence: Factor analytic insights. Presence: Teleoperators and Virtual Environments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.3630},
volume = {10},
year = {2001}
}
@article{Weitz2019,
abstract = {While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility. This development led to an on-going resurgence of the research area of explainable artificial intelligence (XAI) which aims to reduce the opaqueness of those black-box-models. However, much of the current XAI-Research is focused on machine learning practitioners and engineers while omitting the specific needs of end-users. In this paper, we examine the impact of virtual agents within the field of XAI on the perceived trustworthiness of autonomous intelligent systems. To assess the practicality of this concept, we conducted a user study based on a simple speech recognition task. As a result of this experiment, we found significant evidence suggesting that the integration of virtual agents into XAI interaction design leads to an increase of trust in the autonomous intelligent system.},
author = {Weitz, Katharina and Schiller, Dominik and Schlagowski, Ruben and Huber, Tobias and Andr{\'{e}}, Elisabeth},
doi = {10.1145/3308532.3329441},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Weitz et al. - 2019 - Do You Trust Me Increasing User-Trust by Integrating Virtual Agents in Explainable AI Interaction Design.pdf:pdf},
isbn = {978-1-4503-6672-4},
journal = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},
keywords = {acm reference format,deep learning,explainable artificial intelligence,explainable artificial intelligence, interpretabil,human-agent interaction,interpretability,trust,virtual agents},
pages = {7--9},
title = {{"Do You Trust Me?": Increasing User-Trust by Integrating Virtual Agents in Explainable AI Interaction Design}},
year = {2019}
}
@article{Regenbrecht2002,
abstract = {It has long been argued that the possibility to interact in and with a virtual environment (VE) enhances the sense of pres- ence. On the basis of a three-component model of presence, we specify this hypothesis and argue that the mental represen- tation of possible actions should especially enhance spatial presence, and to a lesser extent the involvement and realness of a VE. We support this hypothesis in three studies. A corre- lative study showed that self-reported interaction possibilities correlated significantly with spatial presence, but not with the other two factors. A first experimental study showed that pos- sible self-movement significantly increased spatial presence and realness. A second experimental study showed that even the illusion of interaction, with no actual interaction taking place, significantly increased spatial presence.},
author = {Regenbrecht, Holger and Schubert, Thomas},
file = {:Users/Christoph/Desktop/regenbrecht2002.pdf:pdf},
journal = {Presence: Teleoperators and Virtual Environments},
number = {4},
pages = {425--434},
title = {{Real and Illusory Interactions Enhance Presence}},
volume = {11},
year = {2002}
}
@article{Gunning2016,
abstract = {Based on research into the applications of artificial intelligence (AI) technology in the manufacturing industry in recent years, we analyze the rapid development of core technologies in the new era of 'Internet plus AI', which is triggering a great change in the models, means, and ecosystems of the manufacturing industry, as well as in the development of AI. We then propose new models, means, and forms of intelligent manufacturing, intelligent manufacturing system architecture, and intelligent man-ufacturing technology system, based on the integration of AI technology with information communications, manufacturing, and related product technology. Moreover, from the perspectives of intelligent manufacturing application technology, industry, and application demonstration, the current development in intelligent manufacturing is discussed. Finally, suggestions for the appli-cation of AI in intelligent manufacturing in China are presented.},
author = {Gunning, D},
doi = {10.1111/fct.12208},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Gunning - 2016 - Explainable artificial intelligence (xai) Technical report defense advanced research projects agency darpa-baa-16-53.pdf:pdf},
issn = {20427166},
title = {{Explainable artificial intelligence (xai): Technical report defense advanced research projects agency darpa-baa-16-53}},
year = {2016}
}
@article{DARPA2016,
author = {DARPA},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/DARPA - 2016 - Darpa Xai.pdf:pdf},
title = {{Darpa Xai}},
url = {https://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf{\%}0Ahttps://www.darpa.mil/program/explainable-artificial-intelligence{\%}0Ahttps://www.cc.gatech.edu/{~}alanwags/DLAI2016/(Gunning) IJCAI-16 DLAI WS.pdf},
year = {2016}
}
@article{Lepri2018,
abstract = {The combination of increased availability of large amounts of fine-grained human behavioral data and advances in machine learning is presiding over a growing reliance on algorithms to address complex societal problems. Algorithmic decision-making processes might lead to more objective and thus potentially fairer decisions than those made by humans who may be influenced by greed, prejudice, fatigue, or hunger. However, algorithmic decision-making has been criticized for its potential to enhance discrimination, information and power asymmetry, and opacity. In this paper, we provide an overview of available technical solutions to enhance fairness, accountability, and transparency in algorithmic decision-making. We also highlight the criticality and urgency to engage multi-disciplinary teams of researchers, practitioners, policy-makers, and citizens to co-develop, deploy, and evaluate in the real-world algorithmic decision-making processes designed to maximize fairness and transparency. In doing so, we describe the Open Algortihms (OPAL) project as a step towards realizing the vision of a world where data and algorithms are used as lenses and levers in support of democracy and development.},
author = {Lepri, Bruno and Oliver, Nuria and Letouz{\'{e}}, Emmanuel and Pentland, Alex and Vinck, Patrick},
doi = {10.1007/s13347-017-0279-x},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Lepri et al. - 2018 - Fair, Transparent, and Accountable Algorithmic Decision-making Processes The Premise, the Proposed Solutions, and.pdf:pdf},
issn = {22105441},
journal = {Philosophy and Technology},
keywords = {Accountability,Algorithmic decision-making,Algorithmic transparency,Fairness,Social good},
number = {4},
pages = {611--627},
title = {{Fair, Transparent, and Accountable Algorithmic Decision-making Processes: The Premise, the Proposed Solutions, and the Open Challenges}},
volume = {31},
year = {2018}
}
@article{Bosch2019,
author = {Bosch, Natalie},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Bosch - 2019 - Trust in Autonomous Cars.pdf:pdf},
keywords = {{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_},algorithm transparency,anthropomorphism,autonomous car,communicational interface,mind,predictability,theory of,trust,xai},
title = {{Trust in Autonomous Cars}},
year = {2019}
}
@article{Meteier2019,
author = {Meteier, Quentin and Capallera, Marine and Angelini, Leonardo and Mugellini, Elena and Khaled, Omar Abou and Carrino, Stefano and {De Salis}, Emmanuel and Galland, St{\'{e}}phane and Boll, Susanne},
doi = {10.1145/3349263.3350762},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Meteier et al. - 2019 - Workshop on explainable AI in automated driving.pdf:pdf},
isbn = {9781450369206},
pages = {32--37},
title = {{Workshop on explainable AI in automated driving}},
year = {2019}
}
@article{Doran2018,
abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algorithmic mechanisms; interpretable systems where users can mathematically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
archivePrefix = {arXiv},
arxivId = {1710.00794},
author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
eprint = {1710.00794},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Doran, Schulz, Besold - 2018 - What does explainable AI really mean A new conceptualization of perspectives.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{What does explainable AI really mean? A new conceptualization of perspectives}},
volume = {2071},
year = {2018}
}
@article{Adadi2018,
abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
author = {Adadi, Amina and Berrada, Mohammed},
doi = {10.1109/ACCESS.2018.2870052},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Adadi, Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explainable Artificial Intelligence (XAI).pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Explainable artificial intelligence,black-box models,interpretable machine learning},
pages = {52138--52160},
publisher = {IEEE},
title = {{Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}},
volume = {6},
year = {2018}
}
@article{Holzinger2017,
abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
archivePrefix = {arXiv},
arxivId = {1712.09923},
author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
eprint = {1712.09923},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Holzinger et al. - 2017 - What do we need to build explainable AI systems for the medical domain.pdf:pdf},
number = {Ml},
pages = {1--28},
title = {{What do we need to build explainable AI systems for the medical domain?}},
url = {http://arxiv.org/abs/1712.09923},
year = {2017}
}
@article{Holzinger2018,
abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible 'glass-box' approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
author = {Holzinger, Andreas},
doi = {10.1109/DISA.2018.8490530},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Holzinger - 2018 - From machine learning to explainable AI.pdf:pdf},
isbn = {9781538651025},
journal = {DISA 2018 - IEEE World Symposium on Digital Intelligence for Systems and Machines, Proceedings},
pages = {55--66},
publisher = {IEEE},
title = {{From machine learning to explainable AI}},
year = {2018}
}
@article{Samek2017,
abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
archivePrefix = {arXiv},
arxivId = {1708.08296},
author = {Samek, Wojciech and Wiegand, Thomas and M{\"{u}}ller, Klaus-Robert},
eprint = {1708.08296},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Samek, Wiegand, M{\"{u}}ller - 2017 - Explainable Artificial Intelligence Understanding, Visualizing and Interpreting Deep Learning Models.pdf:pdf},
title = {{Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models}},
url = {http://arxiv.org/abs/1708.08296},
year = {2017}
}
@book{Chen2018,
author = {Chen, Helen},
doi = {10.1007/978-3-319-99740-7},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Chen - 2018 - Measures of Model Interpretability for Model Selection Second IFIP TC 5 , TC 8 WG 8 . 4 , 8 . 9 , TC 12 WG 12 . 9 Inter.pdf:pdf},
isbn = {9783319997407},
keywords = {social networks},
number = {August},
pages = {134--146},
title = {{Measures of Model Interpretability for Model Selection : Second IFIP TC 5 , TC 8 / WG 8 . 4 , 8 . 9 , TC 12 / WG 12 . 9 International Cross-Domain Conference , CD- Measures of Model Interpretability for Model Selection}},
volume = {2},
year = {2018}
}
@article{VanLent2004,
abstract = {As the artificial intelligence (AI) systems in military simulations and computer games become more complex, their actions become increasingly difficult for users to understand. Expert systems for medical diagnosis have addressed this challenge though the addition of explanation generation systems that explain a system's internal processes. This paper describes the AI architecture and associated explanation capability used by Full Spectrum Command, a training system developed for the U.S. Army by commercial game developers and academic researchers.},
author = {{Van Lent}, Michael and Fisher, William and Mancuso, Michael},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Van Lent, Fisher, Mancuso - 2004 - An explainable artificial intelligence system for small-unit tactical behavior.pdf:pdf},
journal = {Proceedings of the National Conference on Artificial Intelligence},
keywords = {Copyright {\textcopyright} 2004 American Association for Artifici},
pages = {900--907},
title = {{An explainable artificial intelligence system for small-unit tactical behavior}},
year = {2004}
}
@article{Liu2017,
abstract = {Interactive model analysis, the process of understanding, diagnosing, and refining a machine learning model with the help of interactive visualization, is very important for users to efficiently solve real-world artificial intelligence and data mining problems. Dramatic advances in big data analytics have led to a wide variety of interactive model analysis tasks. In this paper, we present a comprehensive analysis and interpretation of this rapidly developing area. Specifically, we classify the relevant work into three categories: understanding, diagnosis, and refinement. Each category is exemplified by recent influential work. Possible future research opportunities are also explored and discussed.},
archivePrefix = {arXiv},
arxivId = {1702.01226},
author = {Liu, Shixia and Wang, Xiting and Liu, Mengchen and Zhu, Jun},
doi = {10.1016/j.visinf.2017.01.006},
eprint = {1702.01226},
file = {:Users/Christoph/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2017 - Towards better analysis of machine learning models A visual analytics perspective.pdf:pdf},
issn = {2468502X},
journal = {Visual Informatics},
keywords = {Diagnosis,Interactive model analysis,Interactive visualization,Machine learning,Refinement,Understanding},
number = {1},
pages = {48--56},
publisher = {Elsevier B.V.},
title = {{Towards better analysis of machine learning models: A visual analytics perspective}},
url = {http://dx.doi.org/10.1016/j.visinf.2017.01.006},
volume = {1},
year = {2017}
}
@article{A2004,
author = {Soukoreff, R. William and MacKenzie, I. Scott},
doi = {10.1016/j.ijhcs.2004.09.001},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/Standard{\_}pointing{\_}device{\_}evaluation.pdf:pdf},
journal = {International journal of human-computer studies},
number = {6},
pages = {751--789},
title = {{Towards a standard for pointing device evaluation , perspectives on 27 years of Fitts ' law research in HCI}},
volume = {61},
year = {2004}
}
@article{Roig-maimo,
author = {Roig-maim{\'{o}}, Maria Francesca and MacKenzie, I Scott and Manresa-yee, Cristina and Varona, Javier},
doi = {10.1145/3123818.3123827},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/interaccion2017.pdf:pdf},
isbn = {9781450352291},
keywords = {-  Human-centered computing  -{\textgreater}  User studies,HCI theory, concepts and models,acm reference format,cristina manresa-yee,fitts,head-tracking,i,iso 9241-411,law,maria francesca roig-maim{\'{o}},mobile hci,scott mackenzie,throughput},
title = {{Evaluating Fi s ' Law Performance With a Non-ISO Task}}
}
@article{Mackenzie1989,
author = {MacKenzie, I Scott},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/Information{\_}theoretic{\_}fitts{\_}law.pdf:pdf},
journal = {Journal of motor behavior},
number = {3},
pages = {323--330},
title = {{A Note on the Information-Theoretic Basis for Fitts' Law}},
volume = {21},
year = {1989}
}
@article{Information1954,
author = {Fitts, Paul M.},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/fitts{\_}law{\_}original.pdf:pdf},
journal = {Journal of experimental psychology},
number = {6},
pages = {381},
title = {{The information capacity of the human motor system in controlling the amplitude of movement}},
volume = {47},
year = {1954}
}
@article{Shimada2010,
abstract = {Temporal congruency between the efference copy of a motor command and the reafferent sensory feedback is crucial for identifying self-generated body movements. We investigated how temporal discrepancy between the efference copy and visual feedback affects the self-body movement recognition process. Subjects experienced active and passive hand movements under conditions of delayed visual feedback (118-352 ms) and judged whether observed hand movements were delayed with respect to the felt movement. The results showed that the discrimination threshold of visual feedback delay (50{\%} detection rate) was not significantly different between active and passive movements. In contrast, the judgment probability curve was significantly steeper for active than passive movements. This indicates that the efference copy enhances the contrast between synchronous and asynchronous movements but does not narrow the time window in this process. We discuss processing of active and passive movements in relation to the senses of self-agency and self-ownership.},
author = {Shimada, Sotaro and Qi, Yuan and Hiraki, Kazuo},
doi = {10.1007/s00221-009-2028-6},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/5{\_}Detection{\_}of{\_}visual{\_}feedback{\_}delay.pdf:pdf},
isbn = {0022100920286},
issn = {00144819},
journal = {Experimental Brain Research},
keywords = {Agency,Delayed visual feedback,Efference copy,Ownership,Self-recognition},
number = {2},
pages = {359--364},
title = {{Detection of visual feedback delay in active and passive self-body movements}},
volume = {201},
year = {2010}
}
@article{Akiduki2003,
abstract = {Conflicting inputs from visual and vestibular afferents produce motion sickness and postural instability. However, the relationship of visual and vestibular inputs to each other remains obscure. In this study, we examined the development of subjective sickness- and balance-related symptoms and objective equilibrium ataxia induced by visual-vestibular conflict (VVC) stimulation using virtual reality. The subjective symptoms evaluated by Graybiel's and Hamilton's criteria got gradually worse during the VVC. The objective postural instability was not observed during the VVC, but it occurred immediately after the VVC. There was a time lag between the subjective symptoms and objective ataxia induced by VVC. Our study suggests that the VVC inputs are processed in different pathways causing subjective autonomic symptoms and postural instability in humans. {\textcopyright} 2003 Elsevier Science Ireland Ltd. All rights reserved.},
author = {Akiduki, Hironori and Nishiike, Suetaka and Watanabe, Hiroshi and Matsuoka, Katsunori and Kubo, Takeshi and Takeda, Noriaki},
doi = {10.1016/S0304-3940(03)00098-3},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/akiduki2003.pdf:pdf},
issn = {03043940},
journal = {Neuroscience Letters},
keywords = {Ataxia,Equilibrium,Motion sickness,Virtual reality,Visual-vestibular conflict},
number = {3},
pages = {197--200},
title = {{Visual-vestibular conflict induced by virtual reality in humans}},
volume = {340},
year = {2003}
}
@article{Dummer2009,
abstract = {When a participant views a rubber hand being stroked by a paintbrush while his/her real hand is unseen and similarly stroked by another paintbrush, a misperception known as the rubber hand illusion occurs whereby tactile sensations are falsely referred to the non-body part. The purpose of the current study was to further examine the rubber hand illusion with conditions of movement. An apparatus was devised that would synchronise visual with felt movement in an active condition and a passive condition. An asynchronous condition was included as a control in which visual and felt movement were purposely disconnected. The three movement conditions (active, passive, and asynchronous) were statistically compared in order to assess our prediction that synchronous conditions of movement (especially active) would generate more reports of the illusion. The performance of the movement conditions was evaluated against a visual-tactile condition, which is a known contributor to the rubber hand illusion. Not only significantly more robust reports of the illusion were obtained when visual movement and felt movement were synchronised but there was also a trend toward stronger reports in the active condition rather than the passive condition. Interestingly, the pattern of results differed according to the particular question on the self-report.},
author = {Dummer, Timothy and Picot-Annand, Alexandra and Neal, Tristan and Moore, Chris},
doi = {10.1068/p5921},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/6{\_}Movement{\_}and{\_}the{\_}rubber{\_}hand{\_}illusion.pdf:pdf},
issn = {03010066},
journal = {Perception},
number = {2},
pages = {271--280},
title = {{Movement and the rubber hand illusion}},
volume = {38},
year = {2009}
}
@article{Gonzalez-Franco2017,
abstract = {In Virtual Reality (VR) it is possible to induce illusions in which users report and behave as if they have entered into altered situations and identities. The effect can be robust enough for participants to respond “realistically”, meaning behaviors are altered as if subjects had been exposed to the scenarios in reality. The circumstances in which such VR illusions take place were first introduced in the 80's. Since then, rigorous empirical evidence has explored a wide set of illusory experiences in VR. Here we compile this research and propose a neuroscientific model explaining the underlying perceptual and cognitive mechanisms that enable illusions in VR. Furthermore, we describe the minimum instrumentation requirements to support illusory experiences in VR, and discuss the importance and shortcomings of the generic model.},
author = {Gonzalez-Franco, Mar and Lanier, Jaron},
doi = {10.3389/fpsyg.2017.01125},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/fpsyg-08-01125.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Avatars,Cognition,Embodiment,Perception,Virtual reality},
number = {JUN},
pages = {1--8},
title = {{Model of illusions and virtual reality}},
volume = {8},
year = {2017}
}
@article{Coelho2014,
abstract = {Performing tasks in virtual environments are to increasing extent becoming normal practice; such is possible due to the developments in graphic rendering systems and interaction techniques. Application areas from entertainment to medical industry benefit from gestural 3D interaction. With this in mind, we set out a study aiming to research the relevance of using determined 6DoF input devices in interacting with three-dimensional models in graphical interfaces. In this paper we present an evaluation of 3D pointing tasks using Leap Motion sensor to support 3D object manipulation. Three controlled experiments were performed in the study, exposing test subjects to pointing task evaluations and object deformation, measuring the time taken to perform mesh extrusion and object translation. Qualitative data was gathered using the System Usability Scale questionnaire. The data show a strong correlation between input device and performance time suggesting a dominance of the Leap Motion gestural interface over mouse interaction concerning single target three-dimensional pointing tasks. Multi-target tasks were performed better with mouse interaction due to issues of 3D input system accuracy. Performance time regarding shape deformation task demonstrated that mouse interaction outperformed 3D Input device.},
author = {Coelho, J.C. and Verbeek, F.J},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/coelho-pointing-task-evaluation-of-leap-motion-controller-chisparks2014.pdf:pdf},
isbn = {9789073077553},
journal = {Creating the Difference, Proceedings of the Chi Sparks 2014 Conference},
keywords = {3D environment,3D input device,Author Keywords 3D object manipulation,gesture interaction,leap motion ACM Classification Keywords HCI,pointing task evaluation,user evaluation},
pages = {78--85},
title = {{Pointing Task Evaluation of Leap Motion Controller in 3D Virtual Environment}},
year = {2014}
}
@article{Rietzler2018a,
abstract = {Virtual reality (VR) technology strives to enable a highly im-mersive experience for the user by including a wide variety of modalities (e.g. visuals, haptics). Current VR hardware however lacks a sufficient way of communicating the perception of weight of an object, resulting in scenarios where users can not distinguish between lifting a bowling ball or a feather. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking offsets nudge users to lift their arm higher and result in a visual and haptic perception of weight. We conducted two user studies showing that participants intuitively associated them with the sensation of weight and accept them as part of the virtual world. We further show that compared to no weight simulation, our approach led to significantly higher levels of presence, immersion and enjoyment. Finally, we report perceptional thresholds and offset boundaries as design guidelines for practitioners.},
author = {Rietzler, Michael and Geiselhart, Florian and Gugenheimer, Jan and Rukzio, Enrico},
doi = {10.1145/3173574.3173702},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/paper128.pdf:pdf},
isbn = {9781450356206},
journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI '18},
keywords = {Weight perception,pseudo haptics.,virtual reality},
pages = {1--12},
title = {{Breaking the Tracking: Enabling Weight Perception using Perceivable Tracking Offsets}},
url = {https://doi.org/10.1145/3173574.3173702},
year = {2018}
}
@article{Kasahara2017,
abstract = {We hypothesize that replacing the visual perception of one's body with a spatial-temporal deformed state would change sensations associated with the body. We developed a system that captures full-body movement and generates estimated past and future body movement by deformation. With a head mounted display, people could see their bodies as slightly deformed. We then investigated 1) how human movement is physically changed, and 2) how humans feel about the change in physical and emotional views of the body due to virtual body deformation. Our results show that spatial-temporal deformation of a virtual body actually changes the sense of body as well as physical movement. For instance, a body image generated at approximately 25-100 ms in the future induced a "lighter weight" sensation. On the basis of our findings, we discuss the design implication of computational control for the physical and emotional sense of body.},
author = {Kasahara, Shunichi and Konno, Keina and Owaki, Richi and Nishi, Tsubasa and Takeshita, Akiko and Ito, Takayuki and Kasuga, Shoko and Ushiba, Junichi},
doi = {10.1145/3025453.3025962},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/p6438-kasahara.pdf:pdf},
isbn = {9781450346559},
journal = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI '17},
keywords = {body ownership,embodiment,motion,perception,virtual reality},
pages = {6438--6448},
title = {{Malleable Embodiment: Changing Sense of Embodiment by Spatial-Temporal Deformation of Virtual Human Body}},
url = {http://dl.acm.org/citation.cfm?doid=3025453.3025962},
year = {2017}
}
@article{Tekin,
author = {Tekin, Bugra and Lepetit, Vincent and Fua, Pascal},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/Tekin{\_}Direct{\_}Prediction{\_}of{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
pages = {991--1000},
title = {{Pose Estimation From Motion Compensated}}
}
@article{Faraway2010,
abstract = {A complete scheme for motion prediction based on motion capture data is presented. The scheme rests on three main components: a special posture representation, a diverse motion capture database and prediction method. Most prior motion prediction schemes have been based on posture representations based on well-known local or global angles. Difficulties have arisen when trying to satisfy constraints, such as placing a hand on a target or scaling the posture for a subject of different stature. Inverse kinematic methods based on such angles require optimization that become increasingly complex and computationally intensive for longer linkages. A different representation called stretch pivot coordinates is presented that avoids these difficulties. The representation allows for easy rescaling for stature and other linkage length variations and satisfaction of endpoint constraints, all without optimization allowing for rapid real time use. The validity of this scheme also rests on the availability of motion capture data. There are two situations - in one case the user has access to a larger database of motions relevant to the particular problem while in the second case, the user collects a small amount of motion capture data concerning the task of interest. At the Human Motion Simulation Laboratory (HuMoSim) at the University of Michigan, we have collected several large databases on various types of automobile and materials handling motions. A prediction models based on these databases is presented. Two contrasting prediction methods are demonstrated. One is parametric and uses functional regression analysis to predict the stretch pivot coordinates used in the postural representation as they vary over the time of the motion. This regression-type model allows the use of subject-based variables such as stature and age and task based variables such as target location and object weight to influence the predicted motion. This also allows the scientific study of the effect of such factors using statistical significance testing. The second type of prediction method is based on the idea of nearest neighbor nonparametric regression. A small number (perhaps even just one) of motions is selected that have characteristics similar to the motion we wish to predict. These motions are then averaged in a special way using the stretch pivot coordinates to produce the predicted motion with the required features such as stature, target location etc. This method lends itself to the prediction of motion based on small special purpose motion capture databases collected by the user for some specific problem. Copyright {\textcopyright} 2003 SAE International.},
author = {Faraway, Julian J.},
doi = {10.4271/2003-01-2229},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/03dhm28.pdf:pdf},
journal = {SAE Technical Paper Series},
keywords = {eling,ergonomics,inverse kinematics,motion warping,trajectory mod-},
title = {{Data-Based Motion Prediction}},
volume = {1},
year = {2010}
}
@article{Le2017,
abstract = {Touchscreens are the dominant input mechanism for a variety of devices. One of the main limitations of touchscreens is the latency to receive input, refresh, and respond. This latency is easily perceivable and reduces users' performance. Previous work proposed to reduce latency by extrapolating finger movements to identify future movements-albeit with limited success. In this paper, we propose PredicTouch, a system that improves this extrapolation using inertial measurement units (IMUs). We combine IMU data with users' touch trajectories to train a multi-layer feedforward neural network that predicts future trajectories. We found that this hybrid approach (soft-ware: prediction, and hardware: IMU) can significantly reduce the prediction error, reducing latency effects. We show that using a wrist-worn IMU increases the throughput by 15{\%} for finger input and 17{\%} for a stylus.},
author = {Le, Huy Viet and Schwind, Valentin and G{\"{o}}ttlich, Philipp and Henze, Niels},
doi = {10.1145/3132272.3134138},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/p230-le.pdf:pdf},
isbn = {9781450346917},
journal = {Proceedings of the Interactive Surfaces and Spaces on ZZZ - ISS '17},
pages = {230--239},
title = {{PredicTouch: A System to Reduce Touchscreen Latency using Neural Networks and Inertial Measurement Units}},
url = {http://dl.acm.org/citation.cfm?doid=3132272.3134138},
year = {2017}
}
@article{Rietzler2018,
abstract = {We perceive the flow of time as a constant factor in the real world, but there are examples in media, like films or games, where time is being manipulated and slowed down. Manipulating temporal cues is simple in linear media by slowing down video and audio. Interactive media like VR however poses additional challenges, because user interaction speed is independent from media speed. While the speed of the environment can still be manipulated easily, interaction is a new aspect to consider. We implemented such manipulation by slowing down visual feedback of user movements. In prior experiments we slowed down the virtual representation of a user by applying a velocity based low pass filter and by visually redirecting the motion. We found such a manipulation to be even contributing to realism, enjoyment or presence as long as it is consistent with the experience.},
author = {Rietzler, Michael and Geiselhart, Florian and Brich, Julia and Rukzio, Enrico},
doi = {10.1109/VR.2018.8446136},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/a2-reitzler.pdf:pdf},
isbn = {9781538633656},
journal = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
keywords = {Human computer interaction (HCI),Human-centered computing,Interaction paradigms-Virtual reality},
pages = {773--774},
title = {{Demo of the Matrix Has You: Realizing Slow Motion in Full-Body Virtual Reality}},
year = {2018}
}
@inproceedings{Parcollet2019,
author = {Parcollet, Titouan and Ravanelli, Mirco and Morchid, Mohamed and Trabelsi, Chiheb and Mori, Renato De and Bengio, Yoshua},
booktitle = {ICLR},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/61bfe1883871c0199c6c47572ed9c8f6de9ebe3c.pdf:pdf},
pages = {1--19},
title = {{Quaternion Recurrent Neural Networks}},
year = {2019}
}
@article{Kilteni2013,
abstract = {It has been shown that it is possible to generate perceptual illusions of ownership in immersive virtual reality (IVR) over a virtual body seen from first person perspective, in other words over a body that visually substitutes the person's real body. This can occur even when the virtual body is quite different in appearance from the person's real body. However, investigation of the psychological, behavioral and attitudinal consequences of such body transformations remains an interesting problem with much to be discovered. Thirty six Caucasian people participated in a between-groups experiment where they played a West-African Djembe hand drum while immersed in IVR and with a virtual body that substituted their own. The virtual hand drum was registered with a physical drum. They were alongside a virtual character that played a drum in a supporting, accompanying role. In a baseline condition participants were represented only by plainly shaded white hands, so that they were able merely to play. In the experimental condition they were represented either by a casually dressed dark-skinned virtual body (Casual Dark-Skinned - CD) or by a formal suited light-skinned body (Formal Light-Skinned - FL). Although participants of both groups experienced a strong body ownership illusion towards the virtual body, only those with the CD representation showed significant increases in their movement patterns for drumming compared to the baseline condition and compared with those embodied in the FL body. Moreover, the stronger the illusion of body ownership in the CD condition, the greater this behavioral change. A path analysis showed that the observed behavioral changes were a function of the strength of the illusion of body ownership towards the virtual body and its perceived appropriateness for the drumming task. These results demonstrate that full body ownership illusions can lead to substantial behavioral and possibly cognitive changes depending on the appearance of the virtual body. This could be important for many applications such as learning, education, training, psychotherapy and rehabilitation using IVR.},
author = {Kilteni, Konstantina and Bergstrom, Ilias and Slater, Mel},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/4{\_}Drumming{\_}in{\_}VR.pdf:pdf},
journal = {IEEE transactions on visualization and computer graphics},
number = {4},
pages = {597--605},
title = {{Drumming in immersive virtual reality: the body shapes the way we play}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6479188},
volume = {19},
year = {2013}
}
@article{Banakou2013,
abstract = {An illusory sensation of ownership over a surrogate limb or whole body can be induced through specific forms of multisensory stimulation, such as synchronous visuotactile tapping on the hidden real and visible rubber hand in the rubber hand illusion. Such methods have been used to induce ownership over a manikin and a virtual body that substitute the real body, as seen from first-person perspective, through a head-mounted display. However, the perceptual and behavioral consequences of such transformed body ownership have hardly been explored. In Exp. 1, immersive virtual reality was used to embody 30 adults as a 4-y-old child (condition C), and as an adult body scaled to the same height as the child (condition A), experienced from the first-person perspective, and with virtual and real body movements synchronized. The result was a strong body-ownership illusion equally for C and A. Moreover there was an overestimation of the sizes of objects compared with a nonembodied baseline, which was significantly greater for C compared with A. An implicit association test showed that C resulted in significantly faster reaction times for the classification of self with child-like compared with adult-like attributes. Exp. 2 with an additional 16 participants extinguished the ownership illusion by using visuomotor asynchrony, with all else equal. The size-estimation and implicit association test differences between C and A were also extinguished. We conclude that there are perceptual and probably behavioral correlates of body-ownership illusions that occur as a function of the type of body in which embodiment occurs.},
author = {Banakou, Domna and Groten, Raphaela and Slater, Mel},
doi = {10.1073/pnas.1306779110},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Sitzung - 8/3{\_}Illusory{\_}Ownership.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {31},
pages = {12846--51},
pmid = {23858436},
title = {{Illusory ownership of a virtual child body causes overestimation of object sizes and implicit attitude changes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23858436{\%}0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3732927},
volume = {110},
year = {2013}
}
@article{Ohyama2007,
abstract = {Objective: To examine the development of subjective symptoms and heart rate variability (HRV) during motion sickness induced by virtual reality (VR). Methods: Subjects were 10 healthy young volunteers. During VR immersion, subjects were immersed in a visual-vestibular conflict produced by VR. The levels of the subjective symptoms were assessed by Graybiel's and Hamilton's criteria. HRV was determined by measuring microvascular blood flow or electrocardiogram. Results: Subjective symptoms evaluated by Graybiel's and Hamilton's criteria were gradually worsened during VR. Power spectrum analysis of HRV demonstrated a gradual increase in the low frequency but no change in the high frequency during VR. In this study, individual subjective symptoms were not correlated with the individual result of power spectrum analysis. Conclusion: These findings indicate that there was an increase in sympathetic nervous activity, but no change in parasympathetic nervous activity during motion sickness induced by VR. Given the large inter-individual variability and the reliability of subjective measures, it is not surprising that there is scarcely a relation between the subjective symptoms and the results of power spectrum analysis. {\textcopyright} 2007 Elsevier Ireland Ltd. All rights reserved.},
author = {Ohyama, Seizo and Nishiike, Suetaka and Watanabe, Hiroshi and Matsuoka, Katsunori and Akizuki, Hironori and Takeda, Noriaki and Harada, Tamotsu},
doi = {10.1016/j.anl.2007.01.002},
file = {:Users/Christoph/Documents/Uni/Semester 10$\backslash$:3/Praxisseminar/Projekt/git/PS2019{\_}Gruppe8{\_}FasterThanInRealTime/DOCS/Paper/ohyama2007.pdf:pdf},
issn = {03858146},
journal = {Auris Nasus Larynx},
keywords = {Autonomic function,Heart rate variability,Motion sickness,Virtual reality},
number = {3},
pages = {303--306},
title = {{Autonomic responses during motion sickness induced by virtual reality}},
volume = {34},
year = {2007}
}
@phdthesis{10.5555/143848, author = {MacKenzie, I. Scott}, title = {Fitts’ Law as a Performance Model in Human-Computer Interaction}, year = {1992}, publisher = {University of Toronto}, address = {CAN}, note = {UMI Order No. GAXNN-65985} }

